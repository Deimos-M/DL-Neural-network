{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting house prices : Boston data\n",
    "\n",
    "* It is a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boston House Pricing Dataset\n",
    "Predict the median price of homes in a given Boston suburb in the mid-1970s, given data points about the suburb at the time, such as the crime rate, the local property tax rate, and so on. It has relatively few data points: only 506, split between 404 training samples and 102 test samples. And each feature in the input data (for example, the crime rate) has a different scale. For instance, some values are proportions, which take values between 0 and 1; others take values between 1 and 12, others\n",
    "between 0 and 100, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vipul\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train test split of the dataset\n",
    "(train_data,train_targets),(test_data,test_targets)=boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The trianing dataset consist of 404 observations on 13 different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The test dataset consist of 102 observations on 13 different features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 ----- 50.0\n"
     ]
    }
   ],
   "source": [
    "print(train_targets.min(),'-----',train_targets.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The targets are the median values of owner-occupied homes, in thousands of dollars: (The prices are typically between \\$5,000 and \\$50,000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Data preparation\n",
    "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean=train_data.mean(axis=0)\n",
    "std=train_data.std(axis=0)\n",
    "\n",
    "#normalize train data\n",
    "train_data-=mean\n",
    "train_data /=std\n",
    "\n",
    "#normalize test data\n",
    "test_data-=mean\n",
    "test_data/=std\n",
    "#you should never look into the test data even if you have to do normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Hidden layer 1 => 128 units\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    \n",
    "    # Hidden layer 2 => 64 units\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    \n",
    "    # Output layer with no activation (equivalent to identity activation => for regression problems)\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    # Compile the model (Note the loss function and metric)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Validating using K-fold validation\n",
    "Since we have very less data we can not split training dataset for cross validiation ,it will not make any sense to use about 100 observation for CV and because of these less number of observation the model might show overfitting on CV data. Therefore the best thing to do is to use K-fold validation. Here we are going to use 4-fold validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=4 #no. of folds\n",
    "num_val_sample=len(train_data)//k #size of each fold \n",
    "num_epochs=100\n",
    "all_scores=[] #list of scores of each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold :  1\n",
      "processing fold :  2\n",
      "processing fold :  3\n",
      "processing fold :  4\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    print(\"processing fold : \",i+1)\n",
    "    \n",
    "    #preparing the validation data from partition i\n",
    "    val_data=train_data[i*num_val_sample:(i+1)*num_val_sample]\n",
    "    val_targets=train_targets[i*num_val_sample:(i+1)*num_val_sample]\n",
    "    \n",
    "    #preparing the training data from partition i\n",
    "    partial_train_data=np.concatenate([train_data[:i*num_val_sample:],train_data[(i+1)*num_val_sample:]],axis=0)\n",
    "    partial_train_targets=np.concatenate([train_targets[:i*num_val_sample:],train_targets[(i+1)*num_val_sample:]],axis=0)    \n",
    "    \n",
    "    #using prebuild model\n",
    "    model=build_model()\n",
    "    \n",
    "    test_model=model.fit(partial_train_data,partial_train_targets,validation_data=(val_data,val_targets),batch_size=1,verbose=0)\n",
    "    \n",
    "    # Evaluate the model on the validation data\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    \n",
    "    all_scores.append(val_mae) #appending the score to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.083348321442557, 3.7941452182165465, 3.6494627683469565, 4.640469924058064]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.791856558016031"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The different runs do indeed show rather different validation scores, from 3.08 to 4.64. The average (3.79) is a much more reliable metric than any single score — that’s the entire point of K-fold cross-validation. In this case, you’re off by \\$3,791 on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the final model \n",
    "\n",
    "* you can try to reduce no. of epoch in above code and its been observed that the score is becoming more bad , so let's try to reduce no. of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16d2d6b6940>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a fresh, compiled model\n",
    "model = build_model()\n",
    "\n",
    "# Train it on the entirety of the data\n",
    "# Using batch_size=10\n",
    "model.fit(train_data, train_targets, epochs=40, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test data\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.585851042878394"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final score\n",
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On the test set, we are off by about $2,585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
